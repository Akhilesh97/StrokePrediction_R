df <- download_from_kaggle(local_username, local_api_key, dataset_owner,
download_path, export_path)
head(df)
# Utility Helper
download_from_git <- function(url, download_path) {
# Download file from url
download.file(url, download_path, mode="wb")
# Read csv
# upon quick look at the data set
# if you set smoking_status to factor in col_types, na() won't work
# remove ID, Sex == Other
# output to a factor
df <-
readr::read_csv(download_path, col_types = "cfdfffffddcf", na = c("Unknown", "N/A")) %>%
mutate(smoking_status = factor(smoking_status),
stroke = factor(ifelse(stroke == 1, "yes", "no"),
levels = c("yes", "no"))) %>%
select(-id)
return(df)
}
# Mirror URL
dataset_url <- 'https://raw.githubusercontent.com/KOcasey/Stroke-Diagnosis-Analysis/main/Data/StrokeDiagnosis.csv'
# download path and name
download_filename <- "healthcare-dataset-stroke-data.csv"
download_path <- paste0(dir_data, "/", download_filename)
df <- download_from_git(dataset_url, download_path)
head(df)
knitr::opts_chunk$set(echo = TRUE)
# Kindly import these pacakges to run our project:
library(httr)                # for downloading data
library(tidyverse)           # generic functions and deplyr
library(tidymodels)          # models
library(skimr)               # descriptive stats
library(stringr)             # works with strings
library(themis)              # for SMOTE and other recipes for target balancing
library(vip)                 # for variable importance
library(probably)            # for performance calibration
# Let's set directory paths here...
# Parent dir where our source code is located on your system
dir_root <- getwd()
# Ideally this would be an empty dir location at first
dir_data <- paste0(dir_root, "/data")
# Output location
dir_output <- paste0(dir_root, "/output")
# sanity display
cat("* Your working dir is ---> \n", dir_root)
cat("\n\n* Data is located in ---> \n", dir_data)
cat("\n\n* All the outputs will be generated in ---> \n", dir_output)
knitr::opts_chunk$set(echo = TRUE)
# Kindly import these pacakges to run our project:
library(httr)                # for downloading data
library(tidyverse)           # generic functions and deplyr
library(tidymodels)          # models
library(skimr)               # descriptive stats
library(stringr)             # works with strings
library(themis)              # for SMOTE and other recipes for target balancing
library(vip)                 # for variable importance
library(probably)            # for performance calibration
# Let's set directory paths here...
# Parent dir where our source code is located on your system
dir_root <- getwd()
# Ideally this would be an empty dir location at first
dir_data <- paste0(dir_root, "/data")
# Output location
dir_output <- paste0(dir_root, "/output")
# sanity display
cat("* Your working dir is ---> \n", dir_root)
cat("\n\n* Data is located in ---> \n", dir_data)
cat("\n\n* All the outputs will be generated in ---> \n", dir_output)
library(readr)
library(kaggler)
download_from_kaggle <- function(user, apikey, owner, down_path, exp_path) {
# storing a temp credentials on your system.. (nothing to worry)
kgl_auth(username = user, key = apikey)
# Kaggle API Wrapper, returns a downloadable url
response <- kgl_datasets_download_all(owner_dataset = owner)
# downloading files
download.file(response[["url"]], down_path, mode="wb")
# unzipping downloaded zip file
unzip_result <- unzip(down_path, exdir = exp_path, overwrite = TRUE)
# let's read the csv
filename <- list.files(exp_path, pattern="*.csv", full.names=TRUE)
df <-
readr::read_csv(filename, col_types = "cfdfffffddcf", na = c("Unknown", "N/A")) %>%
mutate(smoking_status = factor(smoking_status),
stroke = factor(ifelse(stroke == 1, "yes", "no"),
levels = c("yes", "no"))) %>%
select(-id)
cat("\nData succesfully downloaed from Kaggle and extracted.")
return(df)
}
# Kaggle Specific credentials
local_username <- "zenbird01"
local_api_key <- "0dbe98a045f8033e05f9d4ef4e3d3b37"
# Source Dataset Author
dataset_owner <- "fedesoriano/stroke-prediction-dataset"
# Download config
download_filename <- "downloaded_data.zip"
download_path <- paste0(dir_data, "/", download_filename)
export_path <- paste0(dir_data, "/dataset")
# let's grab the dataset:
df <- download_from_kaggle(local_username, local_api_key, dataset_owner,
download_path, export_path)
head(df)
# Utility Helper
download_from_git <- function(url, download_path) {
# Download file from url
download.file(url, download_path, mode="wb")
# Read csv
# upon quick look at the data set
# if you set smoking_status to factor in col_types, na() won't work
# remove ID, Sex == Other
# output to a factor
df <-
readr::read_csv(download_path, col_types = "cfdfffffddcf", na = c("Unknown", "N/A")) %>%
mutate(smoking_status = factor(smoking_status),
stroke = factor(ifelse(stroke == 1, "yes", "no"),
levels = c("yes", "no"))) %>%
select(-id)
return(df)
}
# Mirror URL
dataset_url <- 'https://raw.githubusercontent.com/KOcasey/Stroke-Diagnosis-Analysis/main/Data/StrokeDiagnosis.csv'
# download path and name
download_filename <- "healthcare-dataset-stroke-data.csv"
download_path <- paste0(dir_data, "/", download_filename)
df <- download_from_git(dataset_url, download_path)
head(df)
dir.create(file.path(dir_root, "OUT"))
#setwd(file.path(mainDir, subDir))
dir.create(file.path(dir_root, "data"))
#setwd(file.path(mainDir, subDir))
dir.create(dir_data)
knitr::opts_chunk$set(echo = TRUE)
# Packages
#install.packages('httr')
#install.packages('tidyverse')
#install.packages('tidymodels')
#install.packages('skimr')
#install.packages('stringr')
#install.packages('themis')
#install.packages('vip')
#install.packages('probably')
# Kindly import these pacakges to run our project:
library(httr)                # for downloading data
library(tidyverse)           # generic functions and deplyr
library(tidymodels)          # models
library(skimr)               # descriptive stats
library(stringr)             # works with strings
library(themis)              # for SMOTE and other recipes for target balancing
library(vip)                 # for variable importance
library(probably)            # for performance calibration
# Let's set directory paths here...
# Parent dir where our source code is located on your system
dir_root <- getwd()
# Ideally this would be an empty dir location at first
dir_data <- paste0(dir_root, "/data")
dir.create(dir_data)
# Output location
dir_output <- paste0(dir_root, "/output")
dir.create(dir_output)
# sanity display
cat("* Your working dir is ---> \n", dir_root)
cat("\n\n* Data is located in ---> \n", dir_data)
cat("\n\n* All the outputs will be generated in ---> \n", dir_output)
library(readr)
library(kaggler)
download_from_kaggle <- function(user, apikey, owner, down_path, exp_path) {
# storing a temp credentials on your system.. (nothing to worry)
kgl_auth(username = user, key = apikey)
# Kaggle API Wrapper, returns a downloadable url
response <- kgl_datasets_download_all(owner_dataset = owner)
# downloading files
download.file(response[["url"]], down_path, mode="wb")
# unzipping downloaded zip file
unzip_result <- unzip(down_path, exdir = exp_path, overwrite = TRUE)
# let's read the csv
filename <- list.files(exp_path, pattern="*.csv", full.names=TRUE)
df <-
readr::read_csv(filename, col_types = "cfdfffffddcf", na = c("Unknown", "N/A")) %>%
mutate(smoking_status = factor(smoking_status),
stroke = factor(ifelse(stroke == 1, "yes", "no"),
levels = c("yes", "no"))) %>%
select(-id)
cat("\nData succesfully downloaed from Kaggle and extracted.")
return(df)
}
# Kaggle Specific credentials
local_username <- "zenbird01"
local_api_key <- "0dbe98a045f8033e05f9d4ef4e3d3b37"
# Source Dataset Author
dataset_owner <- "fedesoriano/stroke-prediction-dataset"
# Download config
download_filename <- "downloaded_data.zip"
download_path <- paste0(dir_data, "/", download_filename)
export_path <- paste0(dir_data, "/dataset")
# let's grab the dataset:
df <- download_from_kaggle(local_username, local_api_key, dataset_owner,
download_path, export_path)
head(df)
# Utility Helper
download_from_git <- function(url, download_path) {
# Download file from url
download.file(url, download_path, mode="wb")
# Read csv
# upon quick look at the data set
# if you set smoking_status to factor in col_types, na() won't work
# remove ID, Sex == Other
# output to a factor
df <-
readr::read_csv(download_path, col_types = "cfdfffffddcf", na = c("Unknown", "N/A")) %>%
mutate(smoking_status = factor(smoking_status),
stroke = factor(ifelse(stroke == 1, "yes", "no"),
levels = c("yes", "no"))) %>%
select(-id)
return(df)
}
# Mirror URL
dataset_url <- 'https://raw.githubusercontent.com/KOcasey/Stroke-Diagnosis-Analysis/main/Data/StrokeDiagnosis.csv'
# download path and name
download_filename <- "healthcare-dataset-stroke-data.csv"
download_path <- paste0(dir_data, "/", download_filename)
df <- download_from_git(dataset_url, download_path)
head(df)
knitr::opts_chunk$set(echo = TRUE)
# Packages
#install.packages('httr')
#install.packages('tidyverse')
#install.packages('tidymodels')
#install.packages('skimr')
#install.packages('stringr')
#install.packages('themis')
#install.packages('vip')
#install.packages('probably')
# Kindly import these pacakges to run our project:
library(httr)                # for downloading data
library(tidyverse)           # generic functions and deplyr
library(tidymodels)          # models
library(skimr)               # descriptive stats
library(stringr)             # works with strings
library(themis)              # for SMOTE and other recipes for target balancing
library(vip)                 # for variable importance
library(probably)            # for performance calibration
# Let's set directory paths here...
# Parent dir where our source code is located on your system
dir_root <- getwd()
# Ideally this would be an empty dir location at first
dir_data <- paste0(dir_root, "/data")
dir.create(dir_data)
# Output location
dir_output <- paste0(dir_root, "/output")
dir.create(dir_output)
# sanity display
cat("* Your working dir is ---> \n", dir_root)
cat("\n\n* Data is located in ---> \n", dir_data)
cat("\n\n* All the outputs will be generated in ---> \n", dir_output)
library(readr)
library(kaggler)
download_from_kaggle <- function(user, apikey, owner, down_path, exp_path) {
# storing a temp credentials on your system.. (nothing to worry)
kgl_auth(username = user, key = apikey)
# Kaggle API Wrapper, returns a downloadable url
response <- kgl_datasets_download_all(owner_dataset = owner)
# downloading files
download.file(response[["url"]], down_path, mode="wb")
# unzipping downloaded zip file
unzip_result <- unzip(down_path, exdir = exp_path, overwrite = TRUE)
# let's read the csv
filename <- list.files(exp_path, pattern="*.csv", full.names=TRUE)
df <-
readr::read_csv(filename, col_types = "cfdfffffddcf", na = c("Unknown", "N/A")) %>%
mutate(smoking_status = factor(smoking_status),
stroke = factor(ifelse(stroke == 1, "yes", "no"),
levels = c("yes", "no"))) %>%
select(-id)
cat("\nData succesfully downloaed from Kaggle and extracted.")
return(df)
}
# Kaggle Specific credentials
local_username <- "zenbird01"
local_api_key <- "0dbe98a045f8033e05f9d4ef4e3d3b37"
# Source Dataset Author
dataset_owner <- "fedesoriano/stroke-prediction-dataset"
# Download config
download_filename <- "downloaded_data.zip"
download_path <- paste0(dir_data, "/", download_filename)
export_path <- paste0(dir_data, "/dataset")
# let's grab the dataset:
df <- download_from_kaggle(local_username, local_api_key, dataset_owner,
download_path, export_path)
head(df)
# Utility Helper
download_from_git <- function(url, download_path) {
# Download file from url
download.file(url, download_path, mode="wb")
# Read csv
# upon quick look at the data set
# if you set smoking_status to factor in col_types, na() won't work
# remove ID, Sex == Other
# output to a factor
df <-
readr::read_csv(download_path, col_types = "cfdfffffddcf", na = c("Unknown", "N/A")) %>%
mutate(smoking_status = factor(smoking_status),
stroke = factor(ifelse(stroke == 1, "yes", "no"),
levels = c("yes", "no"))) %>%
select(-id)
return(df)
}
# Mirror URL
dataset_url <- 'https://raw.githubusercontent.com/KOcasey/Stroke-Diagnosis-Analysis/main/Data/StrokeDiagnosis.csv'
# download path and name
download_filename <- "healthcare-dataset-stroke-data.csv"
download_path <- paste0(dir_data, "/", download_filename)
df <- download_from_git(dataset_url, download_path)
head(df)
skim(df) %>%
yank("factor")
df <- df %>% filter(gender != "Other")
skim(df) %>%
yank("numeric")
df %>% group_by(stroke, smoking_status) %>%
count()
df %>% filter(is.na(bmi)) %>%
group_by(stroke) %>%
count()
ggplot(df, aes(stroke, age)) +
geom_boxplot(aes(fill = stroke), alpha = 0.5, varwidth = T, notch = T) +
geom_violin(aes(fill = stroke), alpha = 0.5) +
scale_fill_brewer(palette = "Set1", direction = -1) +
xlab("")
ggplot(df, aes(stroke, age)) +
geom_violin(alpha=0.3) +
geom_jitter(alpha=0.2, size=0.8, width = 0.15, height = 0.1, aes(color = gender)) +
geom_boxplot(alpha = 0.2) +
scale_color_brewer(palette = "Set2", direction = -1)
ggplot(df, aes(stroke, bmi)) +
geom_boxplot(aes(fill = stroke), alpha = 0.5, varwidth = T, notch = T) +
geom_violin(aes(fill = stroke), alpha = 0.5) +
scale_fill_brewer(palette = "Set1", direction = -1) +
xlab("")
facet_names <- c("no" = "no stroke", "yes" = "stroke")
ggplot(df, aes(age, bmi)) +
geom_point(color = "steelblue", alpha = 0.8, size = 0.5) +
facet_grid(rows = vars(stroke), labeller = as_labeller(facet_names)) +
guides()
ggplot(df, aes(age, avg_glucose_level)) +
geom_point(aes(color = smoking_status), alpha = 0.6, size = 1) +
scale_fill_brewer(palette = "Set1", direction = -1) +
facet_grid(rows = vars(stroke), labeller = as_labeller(facet_names)) +
guides()
ggplot(df, aes(smoking_status, age)) +
geom_boxplot(aes(fill = stroke), alpha = 0.5, varwidth = T, notch = T) +
scale_fill_brewer(palette = "Set1", direction = -1) +
xlab("")
ggplot(df, aes(avg_glucose_level, bmi)) +
geom_point(aes(color = age), alpha = 0.6, size = 1) +
scale_fill_brewer(palette = "Set1", direction = -1) +
facet_grid(rows = vars(stroke), labeller = as_labeller(facet_names)) +
guides() +
xlab("avg glucose level")
gender <- df %>% group_by(stroke, gender) %>% summarize(N=n())
ggplot(gender, aes(stroke, N)) +
geom_bar(aes(fill=gender), alpha = 0.8, stat = "identity", position = "fill") +
scale_fill_brewer(palette = "Set2", direction = -1) +
ylab("proportion")
hyptens <- df %>% group_by(stroke, hypertension) %>% summarize(N = n())
ggplot(hyptens, aes(stroke, N)) +
geom_bar(aes(fill = hypertension), alpha = 0.8, stat = "identity", position = "fill") +
scale_fill_brewer(palette = "Set2", direction = -1) +
ylab("proportion")
heart <- df %>% group_by(stroke, heart_disease) %>% summarize(N=n())
ggplot(heart, aes(stroke, N)) +
geom_bar(aes(fill = heart_disease), alpha = 0.8, stat = "identity", position = "fill") +
scale_fill_brewer(palette = "Set2", direction = 1) +
ylab("proportion")
married <- df %>% group_by(stroke, ever_married) %>% summarize(N=n())
ggplot(married, aes(stroke, N)) +
geom_bar(aes(fill = ever_married), alpha = 0.8, stat = "identity", position = "fill") +
scale_fill_brewer(palette = "Set2", direction = -1) +
ylab("proportion")
work <- df %>% group_by(stroke, work_type) %>% summarize(N=n())
ggplot(work, aes(stroke, N)) +
geom_bar(aes(fill = work_type), alpha = 0.8, stat = "identity", position = "fill") +
scale_fill_brewer(palette = "Set2", direction = 1) +
ylab("proportion")
residence <- df %>% group_by(stroke, Residence_type) %>% summarize(N=n())
ggplot(residence, aes(stroke, N)) +
geom_bar(aes(fill = Residence_type), alpha = 0.8, stat = "identity", position = "fill") +
scale_fill_brewer(palette = "Set2", direction = 1) +
ylab("proportion")
smoking <- df %>% group_by(stroke, smoking_status) %>% summarize(N=n())
ggplot(smoking, aes(stroke, N)) +
geom_bar(aes(fill = smoking_status), alpha = 0.8, stat = "identity", position = "fill") +
scale_fill_brewer(palette = "Set2", direction = 1) +
ylab("proportion")
df %>% filter(work_type == "children") %>%
group_by(smoking_status) %>%
summarise(N = n(),
avg.age = mean(age),
max.age = max(age),
min.age = min(age))
set.seed(124)
data_split <- initial_split(df, prop = 3/4, strata = stroke)
df_train <- training(data_split)
df_test <- testing(data_split)
set.seed(345)
# Stratified, repeated 10-fold cross-validation
cv_folds <- vfold_cv(df_train, strata = "stroke", v = 10, repeats = 10)
# metrics
cls_metrics <- metric_set(roc_auc, j_index)
prep_recipe <- recipe(stroke ~ ., data = df_train) %>%
step_impute_bag(bmi, smoking_status) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
#step_impute_bag(all_predictors()) %>%
step_normalize(age, avg_glucose_level, bmi) %>%
step_smote(stroke, over_ratio = 1, seed = 100) %>%
check_missing(all_predictors()) %>%
step_zv(all_predictors())
prep_recipe
# recipe for LR
lr_recipe <- prep_recipe %>%
step_corr(all_predictors(), threshold = 0.75)
# set model type/engine
lr_mod <- logistic_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet")
# define the workflow
lr_workflow <-
workflow() %>%
add_model(lr_mod) %>%
add_recipe(lr_recipe)
# create a tune grid
lr_reg_grid <- tibble(penalty = 10**seq(-4, 0, length.out = 30))
# train and tune the model
lr_res <- tune_grid(lr_workflow,
grid = lr_reg_grid,
resamples = cv_folds,
control = control_grid(save_pred = TRUE),
metrics = cls_metrics)
install.packages('glmnet')
# recipe for LR
lr_recipe <- prep_recipe %>%
step_corr(all_predictors(), threshold = 0.75)
# set model type/engine
lr_mod <- logistic_reg(penalty = tune(), mixture = 1) %>%
set_engine("glmnet")
# define the workflow
lr_workflow <-
workflow() %>%
add_model(lr_mod) %>%
add_recipe(lr_recipe)
# create a tune grid
lr_reg_grid <- tibble(penalty = 10**seq(-4, 0, length.out = 30))
# train and tune the model
lr_res <- tune_grid(lr_workflow,
grid = lr_reg_grid,
resamples = cv_folds,
control = control_grid(save_pred = TRUE),
metrics = cls_metrics)
autoplot(lr_res)
lr_res
lr_res
lr_best$penalty
lr_res
show_best(lr_res)
show_best(lr_res)$penalty
show_best(lr_res)
show_best(lr_res)[0]$penalty
show_best(lr_res)
show_best(lr_res)$penalty[0]
show_best(lr_res)$penalty
show_best(lr_res)$penalty[1]
k = show_best(lr_res)$penalty[1]
k = show_best(lr_res)$penalty[1]
k
# Final model
# - with tuned hyper-params retireved from GridTuning on penalty terms:
last_mod <- logistic_reg(penalty = k, mixture = 1) %>%
set_engine("glmnet")  %>%
set_mode("classification")
# the last workflow: based on LR
last_wf <-
lr_workflow %>%
update_model(last_mod)
# Final fitting:
set.seed(345)
last_fit <-
last_wf %>%
last_fit(data_split)
last_fit
last_fit
plot(lr_res)
summary(lr_res)
k = show_best(lr_res)$penalty[1]
k
conf_mat_resampled
conf_mat_resampled(last_fit)
conf_mat_resampled(last_fit, tidy = FALSE)
conf_mat_resampled(last_fit, tidy = FALSE)
conf <- conf_mat_resampled(last_fit, tidy = FALSE)
conf
conf['yes']
conf[1]
conf[1][1]
conf[1][1]
conf[0][1]
conf
precision(last_fit)
precision(last_fit, data_split)
conf
conf_mat
conf_mat(last_fit)
